# Predicting-Task-Drivern-Attention
Predicting Task-driven Attention via Integrating Bottom-up Stimulus and Top-down Guidance

This project is built for our newly-submitted paper:
Zhixiong Nan, Jingjing Jiang, Xiaofeng Gao, Sanping Zhou, Weiliang Zuo, Ping Wei and Nanning Zheng. Predicting Task-driven Attention via Integrating Bottom-up Stimulus and Top-down Guidance. Submitted to IEEE Transactions on Image Processing, 2021. 

1.	Code

The code is named as “Predicting Task-Drivern Attention.zip”, which can be directly downloaded in this project.

2.	Datasets

CAD120 dataset is introduced in the paper “Learning Human Activities and Object Affordances from RGB-D Videos”, with the link of https://www.researchgate.net/publication/231609161_Learning_Human_Activities_and_Object_Affordances_from_RGB-D_Videos
The paper provides the link for downloading the CAD120 dataset.

TIA dataset is introduced in the paper “Where and why are they looking? jointly inferring human attention and intentions in complex tasks”, with the link of https://www.researchgate.net/publication/329740365_Where_and_Why_are_They_Looking_Jointly_Inferring_Human_Attention_and_Intentions_in_Complex_Tasks
Please contact the authors of this paper to get access of the TIA dataset.

3.	Annotations

We manually annotated the attention objects in the above two datasets. 
The annotation for CAD120 dataset can be downloaded at the following link:
https://pan.baidu.com/s/1sp0ECVGRSAashLgJc50Zug 
password: jzdh

The annotation for TIA dataset can be downloaded at the following link:
https://pan.baidu.com/s/1YOU3BVIED3_vUif5_lsbyQ
password:7z7s 

